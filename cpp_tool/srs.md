- 音视频开发

音视频并不是一个标准和协议，在不同行业中，音视频是完全不同的技术。不同行业要解决的核心问题不同，导致了不同体系的音视频技术,
- 直播行业主要应用的是:rtmp,flv,HLS; --- 要解决的问题是高清，流畅
- 通话行业使用的更多的是webrtc; --- 解决的延迟关键，声学
- 监控安防:rtsp,gb28181,ONVIF; --- 解决随时看到之前的内容，物体识别

各个行业需要就借助互联网的力量实现数字化，--- 电商--->直播电商;

音视频是一种能力，各个行业都在探索，通过音视频实现增值;--- 数字化的能力;

srs的核心要解决的问题：**解决流媒体的网关** 


# mac运行srs
- lldb调试
```shell
lldb -- ./objs/srs -c conf/console.conf
b SrsConfig::parse_options
bt
f
p argc
p argv
p argv[0]
p argv[1]
p argv[2]
b 1907
p config_file

```
- 解决配置文件兼容性的函数
```
src_config_transform_vhost(root)
```

# srs做什么
- 流媒体网关，填协议转换的坑，小微场景的应用.

### WebRTC标准和架构
WebRTC不只有一个RFC,核心RFC大概有WebRTC,DTLS,SRTP,RTP,STUN,以及各种扩展。

# srs(流媒体网关)解决什么问题
互联网音视频是主要RTMP接入，逐步也支持SRT和WebRTC,而观看则主要是HLS和WebRTC.
只有媒体网关，才能再各种场景中，适配其他行业的标准，或者转换不同推流协议。
技术先进不是决定性的，能服务好业务的协议就有自己的生存空间。

# srs应用的语言技术
- c++98,减少功能，思考最佳结构，本质上都是人的问题。
- Lightweight Thread 音视频是C的生态，解决固有难题的复杂性，只有通过轻量线程.
- JS的扩展，是有利于社区贡献的最佳方式，架构上必须支持扩展。


# srs支持的协议
```
RTMP推流--------------|     |------------RTMP播放
WebRTC推流------------|     |------------HLS播放
GB-28181推流----------|     |------------HTTP-FLV播放
SRT推流---------------| SRS |------------WebRTC播放
MPEG-TS推流-----------|     |------------HDS播放
HTTP-FLV推流----------|     |------------MP4/FLV录制
FFmpeg拉RTSP----------|
```
- RTMP是最流行的直播协议。
- HLS是苹果的协议，缺点是延迟高，严格上讲不是一个流协议，而是一个文件协议，服务器会将音视频数据打包成一小段一线段的ts切片
- HTTP-FLV（无插件播放）不是一个标准的w3c标准的流媒体协议，利用了http Chunked特性，然后HTTP Body数据类型就是FLV跟RTMP的类型差不多，对于服务器来说处理容易，平滑的切换
浏览器接收到数据后，用flv.js这个库重新组装数据，通过MSE给到浏览器，从而实现了视频播放的功能。
- WebRTC无插件播放，目前最火的流媒体协议，对于前端开发者更加方便，几行代码就可以实现采集，渲染，编解码，缺点是底层技术栈太难。
- 安防类协议RTSP是与RTMP一字之差，延迟低，两者的主要区别是RTMP的Payload是FLV,而RTSP得payload是RTP，最主要得区别还是应用场景，RTSP主要是在内网使用，因为每个摄像头是可以把他理解成一个RTSP得server,如果看视频得化，主动去摄像头拉流。摄像头是不会主动推流的。所以主要的问题是处在公网的服务器或者客户端时无法看到内网的摄像头的视频。
- GB-28181协议，是公安一所联合摄像头厂家定制的协议, 专门适用于安防领域的协议。指定这个协议的目的是为了解决RTSP的问题，公网的服务器无法访问内网的摄像头，所以GB28181协议就是一个摄像头 主动注册，并且向外推流的协议，有两个版本，一个是2011年的UDP协议，后来发现上了公网后丢包很严重，又做了补充协议，改成了TCP,互联网上面的协议最开始是TCP,逐渐演进到现在是UDP(比如webrtc),安防是在局域网使用，网络条件好，采用UDP传输，后边上云后，发现丢包严重，各种卡顿，花屏，然后做了一个补充协议，所以安防做了一个适用于本地存储的，或者是局域网传输的PS打包，而不是选择适用于网络传输的TS打包，历史原因。PS打包比较复杂，首先是可变长度，被丢包后很难恢复，里面有一些扩展字段，每个厂家理解不一致，从而引起一些兼容性问题，还有一些厂家喜欢加点私货，所以目前SRS目前会出现decode ps data error这种错误。

# 使用场景
- 视频直播，主播通过OBS推流到SRS，转封装成HTTP-FLV流，转封装成HLS;Players根据平台的播放器可以选HTTP-FLV或HLS流播放；
- WebRTC通话业务，多人通话，视频会议电子白板;
- 安防监控领域，使用FFmpeg拉取RTSP再转RTMP推送到SRS,或者使用GB28181协议直接推流，Players可以使用WebRTC播放;
- 广电领域，可以使用SRT协议推流

# 安防云边端 
- 云端：调度系统；指令系统，数据系统，SRS
- 边缘端：视频网关，边缘负责拉流然后转推到云端；SDK接入 RTSP接入 GB28181接入-----------> 实时预览，历史回放   设备管理，报警上报 协议转换，事件联动| 抓图下载,指令控制
- 设备端：IPC DVR NVR; 球机墙机红外机，

# 流媒体
![image.png](https://s2.loli.net/2022/07/21/MsuHRX9EGaK8SAv.png)
- 摄像头多了需要有一个下级服务器，类似于IP网络中的NAT子网
# 安防特殊流程
![image.png](https://s2.loli.net/2022/07/21/dTknV9EqwNe7BCb.png)
- 码率随着播放时间的减小，也是成倍增加的，1080P是4兆的码流，2倍速码率就变成了8兆，四倍速就是16兆（sleep时间少了）；cpu的消耗也是增加的，安防就是有初帧操作，不能把I帧扔掉，否则P帧就不能接触来了。根据倍速，I帧的时间间隔，有一个抽帧的算法。减轻带宽的压力，减轻解码的压力，慢放就是时间间隔加长就好了。

# 延迟问题
- 采集渲染：硬件设备，主要延迟几毫秒，到十几毫秒
- 编码和解码：编码类型：I帧，P帧，B帧 编码格式：AVC,HEVC,VP8,VP9,AVI等；I帧时间间隔；B帧会带来额外的延迟，相对于I,P帧来说是一个双向预测，可能会参考前面和后面的帧，客户端解码的时候也是一样，收到一个B帧后，可能也是解不出来，必须等到参考帧到达后才能解码，所以就造成了可能会有几十毫秒的延迟，加起来就可能有100多毫秒的延迟。H265是H264的升级版，压缩率会提高大概30%左右，更高的编码，更高的压缩率带来的就是更长的编码时长，I帧间隔会对网络造成冲击，因为I帧这个包比较大,会造成网络抖动，应用层(HLS RTMP HTTP-FLV)延迟最高的就是HLS,传输层(UDP和TCP);
- 网络传输：传输协议，应用层和传输层，缓存，服务端和播放端
- 性能优化：合并读写; 缓存350ms的一个数据，就能提升差不多一倍的并发，降低延迟，可以将缓存关掉。

# srs不支持的协议
- onvif 
- RTSP
<br>这两个协议都是局域网协议，处在公网的SRS是没有办法使用这两个协议，
---

# SRS的docker启动
- [参考](https://github.com/ossrs/dev-docker/tree/aarch64) 

